{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_steps(\n",
    "    model_name: str,\n",
    "    chunks: list,\n",
    "    next_sentences: list,\n",
    "    seq_length: int = 1024,\n",
    "    batch_size: int = 256,\n",
    "    epochs: int = 3,\n",
    "    learning_rate: float = 1e-5,\n",
    "    device: str = \"cuda:1\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune a language model using LoRA for multiple epochs.\n",
    "    :param epochs: Number of training epochs.\n",
    "    \"\"\"\n",
    "    print(f\"Model: {model_name}\")\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add padding token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare dataset and dataloader\n",
    "    dataset = EBAE_EBARDataset(chunks, next_sentences, tokenizer, seq_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    max_grad_norm = 1500.0  # Maximum gradient norm\n",
    "    gradient_accumulation_steps = 8  # Simulates a larger batch size\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    losses = []\n",
    "    gradient_norms = []\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        epoch_gradient_norms = []\n",
    "        loss_buffer = []\n",
    "\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            next_input_ids = batch[\"next_input_ids\"].to(device)\n",
    "\n",
    "            # Compute loss\n",
    "            try:\n",
    "                loss = ebae_ebar_loss(model, input_ids, next_input_ids, tokenizer, device)\n",
    "                if loss is None:\n",
    "                    print(f\"Skipping batch {step} due to None loss.\")\n",
    "                    continue\n",
    "            except ValueError as e:\n",
    "                print(f\"Error at batch {step}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Normalize loss for accumulation\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            loss_buffer.append(loss.item())\n",
    "\n",
    "            # Update after accumulation\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                # Clip gradients and step optimizer\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                epoch_gradient_norms.append(grad_norm)\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Track losses\n",
    "                avg_loss = sum(loss_buffer) / gradient_accumulation_steps\n",
    "                epoch_loss += avg_loss\n",
    "                loss_buffer = []  # Reset for next cycle\n",
    "\n",
    "                print(f\"Step {step + 1}, Avg Loss: {avg_loss:.4f}, Grad Norm: {grad_norm:.4f}\")\n",
    "\n",
    "        # Handle leftover gradients\n",
    "        if loss_buffer:\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            epoch_gradient_norms.append(grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            avg_loss = sum(loss_buffer) / len(loss_buffer)  # Average remaining losses\n",
    "            epoch_loss += avg_loss\n",
    "\n",
    "        # Record epoch stats\n",
    "        losses.append(epoch_loss)\n",
    "        gradient_norms.append(epoch_gradient_norms)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} Loss: {epoch_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch + 1} Gradient Norms: {epoch_gradient_norms}\")\n",
    "\n",
    "    # Save model and tokenizer\n",
    "    output_dir = \"./ebae-model\"\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "    # Save metrics\n",
    "    torch.save({\"losses\": losses, \"gradient_norms\": gradient_norms}, \"training_metrics.pth\")\n",
    "    print(\"Metrics saved to training_metrics.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
