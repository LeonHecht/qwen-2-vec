{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all texts from Spanish Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load texts (all wikipedia articles in spanish) using pickle\n",
    "import pickle\n",
    "\n",
    "with open(\"wiki_texts_list.pkl\", \"rb\") as f:\n",
    "    texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut texts to the first 1000, so that the training does not take prohibitly long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = texts[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk Wikipedia texts into chunks of sequence length 1024 for EBAE (using Tokenizer spezified in model_name) and the next sentences for EBAR and dump it to a pickle file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def create_ebae_ebar_chunks(texts, tokenizer, pre_seq_length=1000, train_seq_len=1024):\n",
    "    \"\"\"\n",
    "    Creates chunks for EBAE and EBAR by separating prompts and next sentences.\n",
    "    \n",
    "    :param texts: List of texts to process.\n",
    "    :param tokenizer: Tokenizer object for tokenizing the text.\n",
    "    :param pre_seq_length: Maximum length of the prompt chunk before adding the next sentence.\n",
    "    :param train_seq_len: Maximum sequence length for training.\n",
    "    :return: Two lists: prompts (EBAE input) and next sentences (EBAR input).\n",
    "    \"\"\"\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add padding token\n",
    "    prompts = []\n",
    "    next_sentences = []\n",
    "    \n",
    "    for text in tqdm(texts):\n",
    "        sentences = text.split(\".\")  # Split text into sentences\n",
    "        input_buffer = []\n",
    "        token_count = 0\n",
    "\n",
    "        # Batch tokenize sentences\n",
    "        batch_tokens = tokenizer(\n",
    "            sentences,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        for i, (tokens, sentence) in enumerate(zip(batch_tokens, sentences)):\n",
    "            current_token_length = (tokens != tokenizer.pad_token_id).sum().item()  # Count non-padding tokens\n",
    "\n",
    "            # Skip sentences that are too long individually\n",
    "            if current_token_length > pre_seq_length:\n",
    "                print(\"Skipping sentence as it exceeds seq_length\")\n",
    "                continue\n",
    "\n",
    "            if token_count + current_token_length <= pre_seq_length:\n",
    "                # Add the sentence if it fits\n",
    "                input_buffer.append(sentence)\n",
    "                token_count += current_token_length\n",
    "            else:\n",
    "                # Add the current chunk to the list and reset buffer\n",
    "                if input_buffer and i < len(sentences) - 1:  # Ensure there's a next sentence\n",
    "                    prompts.append(\" \".join(input_buffer))\n",
    "                    next_sentences.append(sentences[i])  # Use the next sentence for EBAR\n",
    "                input_buffer = [sentence]\n",
    "                token_count = current_token_length\n",
    "\n",
    "        # Handle leftover sentences in the buffer\n",
    "        if input_buffer and len(input_buffer) < len(sentences):\n",
    "            prompts.append(\" \".join(input_buffer))\n",
    "            next_sentences.append(sentences[len(input_buffer)])  # Use the next available sentence\n",
    "\n",
    "    # Validate all chunks and remove invalid pairs\n",
    "    valid_prompts, valid_next_sentences = [], []\n",
    "    for idx in range(len(prompts)):\n",
    "        prompt_tokens = tokenizer(prompts[idx])[\"input_ids\"]\n",
    "        next_tokens = tokenizer(next_sentences[idx])[\"input_ids\"]\n",
    "\n",
    "        if len(prompt_tokens) <= train_seq_len - 30 and len(next_tokens) <= train_seq_len - 30:\n",
    "            valid_prompts.append(prompts[idx])\n",
    "            valid_next_sentences.append(next_sentences[idx])\n",
    "        else:\n",
    "            print(f\"Pair {idx} is too long after processing (prompt: {len(prompt_tokens)} tokens, next: {len(next_tokens)} tokens).\")\n",
    "\n",
    "    return valid_prompts, valid_next_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon/tesis/spanish-legal-ir/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  4%|▍         | 44/1000 [00:01<00:36, 26.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sentence as it exceeds seq_length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 146/1000 [00:03<00:15, 56.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sentence as it exceeds seq_length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 158/1000 [00:03<00:32, 26.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sentence as it exceeds seq_length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 167/1000 [00:04<00:38, 21.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sentence as it exceeds seq_length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 671/1000 [00:15<00:07, 46.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sentence as it exceeds seq_length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 704/1000 [00:16<00:06, 43.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sentence as it exceeds seq_length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 769/1000 [00:17<00:05, 39.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sentence as it exceeds seq_length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 834/1000 [00:19<00:04, 35.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sentence as it exceeds seq_length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 917/1000 [00:20<00:01, 74.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sentence as it exceeds seq_length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 939/1000 [00:21<00:01, 39.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sentence as it exceeds seq_length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:22<00:00, 43.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sentence as it exceeds seq_length\n",
      "Error: Pair 453 is too long after processing (prompt: 996 tokens, next: 3 tokens).\n",
      "Error: Pair 543 is too long after processing (prompt: 996 tokens, next: 4 tokens).\n",
      "Error: Pair 1812 is too long after processing (prompt: 996 tokens, next: 13 tokens).\n",
      "Error: Pair 1825 is too long after processing (prompt: 999 tokens, next: 6 tokens).\n",
      "Error: Pair 1826 is too long after processing (prompt: 1000 tokens, next: 18 tokens).\n",
      "Error: Pair 2058 is too long after processing (prompt: 1013 tokens, next: 16 tokens).\n",
      "Error: Pair 2069 is too long after processing (prompt: 1029 tokens, next: 2 tokens).\n",
      "Error: Pair 2765 is too long after processing (prompt: 1002 tokens, next: 7 tokens).\n",
      "Error: Pair 3092 is too long after processing (prompt: 1015 tokens, next: 6 tokens).\n",
      "Error: Pair 3093 is too long after processing (prompt: 1051 tokens, next: 6 tokens).\n",
      "Error: Pair 3094 is too long after processing (prompt: 1068 tokens, next: 3 tokens).\n",
      "Error: Pair 3095 is too long after processing (prompt: 1072 tokens, next: 4 tokens).\n",
      "Error: Pair 3144 is too long after processing (prompt: 1015 tokens, next: 17 tokens).\n",
      "Error: Pair 3145 is too long after processing (prompt: 1011 tokens, next: 15 tokens).\n",
      "Error: Pair 3877 is too long after processing (prompt: 92 tokens, next: 1089 tokens).\n",
      "Error: Pair 3922 is too long after processing (prompt: 1001 tokens, next: 3 tokens).\n",
      "Number of chunks: 5595\n",
      "Number of next sentences: 5595\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Generate sentence pairs\n",
    "chunks, next_sentences = create_ebae_ebar_chunks(texts, tokenizer, pre_seq_length=900, train_seq_len=1024)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Number of next sentences: {len(next_sentences)}\")\n",
    "\n",
    "# save chunks using pickle\n",
    "import pickle\n",
    "\n",
    "with open(\"wiki_chunks_list_ebae_ebar.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n",
    "\n",
    "with open(\"wiki_next_sentences_list_ebae_ebar.pkl\", \"wb\") as f:\n",
    "    pickle.dump(next_sentences, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
