{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all texts from Spanish Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load texts (all wikipedia articles in spanish) using pickle\n",
    "import pickle\n",
    "\n",
    "with open(\"wiki_texts_list.pkl\", \"rb\") as f:\n",
    "    texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut texts to the first 1000, so that the training does not take prohibitly long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = texts[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk Wikipedia texts into chunks of sequence length 1024 (using Tokenizer spezified in model_name) and dump it to a pickle file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def create_ebae_chunks(texts, tokenizer, pre_seq_length=1000, train_seq_len=1024):\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add padding token\n",
    "    chunks = []\n",
    "    \n",
    "    for text in tqdm(texts):\n",
    "        sentences = text.split(\".\")  # Split text into sentences\n",
    "        input_buffer = []\n",
    "        token_count = 0\n",
    "\n",
    "        # Batch tokenize sentences\n",
    "        batch_tokens = tokenizer(\n",
    "            sentences,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        for tokens, sentence in zip(batch_tokens, sentences):\n",
    "            current_token_length = (tokens != tokenizer.pad_token_id).sum().item()  # Count non-padding tokens\n",
    "\n",
    "            # Skip sentences that are too long individually\n",
    "            if current_token_length > pre_seq_length:\n",
    "                print(f\"Skipping sentence as it exceeds seq_length: {sentence[:50]}...\")\n",
    "                continue\n",
    "\n",
    "            if token_count + current_token_length <= pre_seq_length:\n",
    "                # Add the sentence if it fits\n",
    "                input_buffer.append(sentence)\n",
    "                token_count += current_token_length\n",
    "            else:\n",
    "                # Add the current chunk to the list and reset buffer\n",
    "                if input_buffer:\n",
    "                    chunks.append(\" \".join(input_buffer))\n",
    "                input_buffer = [sentence]\n",
    "                token_count = current_token_length\n",
    "\n",
    "        # Handle leftover sentences in the buffer\n",
    "        if input_buffer:\n",
    "            chunks.append(\" \".join(input_buffer))\n",
    "\n",
    "    # Validate all chunks and remove long ones\n",
    "    idx = 0\n",
    "    while idx < len(chunks):\n",
    "        tokenized_chunk = tokenizer(chunks[idx])[\"input_ids\"]\n",
    "        if len(tokenized_chunk) > train_seq_len - 10:\n",
    "            print(f\"Error: Chunk {idx} is too long after processing ({len(tokenized_chunk)} tokens).\")\n",
    "            chunks.pop(idx)  # Remove the chunk and do not increment the index\n",
    "        else:\n",
    "            idx += 1  # Only increment if no removal happened\n",
    "\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Generate sentence pairs\n",
    "chunks = create_ebae_chunks(texts, tokenizer, pre_seq_length=900, train_seq_len=1024)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "\n",
    "# save chunks using pickle\n",
    "import pickle\n",
    "\n",
    "with open(\"wiki_chunks_list_ebae.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
